{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c747fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from src.tasks import Sine_Task, Sine_Task_Distribution\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1021a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40,40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40,1))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        x = nn.functional.linear(x, weights[0], weights[1])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[2], weights[3])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[4], weights[5])\n",
    "        return x\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79fb18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMAML():\n",
    "    def __init__(self, model, tasks, inner_lr, meta_lr, K=10, inner_steps=1, tasks_per_meta_batch=1000, radius=0.05, num_ve_iterations=5):\n",
    "        \n",
    "        # important objects\n",
    "        self.tasks = tasks\n",
    "        self.model = model\n",
    "        self.weights = list(model.parameters()) # the maml weights we will be meta-optimising\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = torch.optim.Adam(self.weights, meta_lr)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps # with the current design of MAML, >1 is unlikely to work well \n",
    "        self.tasks_per_meta_batch = tasks_per_meta_batch \n",
    "        self.radius = radius\n",
    "        self.num_ve_iterations = num_ve_iterations\n",
    "        \n",
    "        # metrics\n",
    "        self.plot_every = 10\n",
    "        self.print_every = 10\n",
    "        self.save_every = 100\n",
    "        self.meta_losses = []\n",
    "    \n",
    "    def inner_loop(self, task, temp_weights, compute_loss=False):\n",
    "        # perform training on data sampled from task\n",
    "    
    "        X, y = task.sample_data(self.K)\n",
    "        if compute_loss:\n",
    "            loss = self.criterion(self.model.parameterised(X, temp_weights), y) / self.K\n",
    "        else:\n",
    "            for step in range(self.inner_steps):\n",
    "                loss = self.criterion(self.model.parameterised(X, temp_weights), y) / self.K\n",
    "                \n",
    "                # compute grad and update inner loop weights\n",
    "                grad = torch.autograd.grad(loss, temp_weights)\n",
    "                temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "                        \n",
    "            loss = 0.\n",
    "            \n",
    "        return temp_weights, loss\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for iteration in range(1, num_iterations+1):\n",
    "            \n",
    "            # compute meta loss\n",
    "            meta_loss = 0\n",
    "            task_weights_list = [[w.clone() for w in self.weights] for _ in range(self.tasks_per_meta_batch)]\n",
    "\n",
    "            tasks_list = []\n",
    "            for i in range(self.tasks_per_meta_batch):\n",
    "                tasks_list.append(self.tasks.sample_task())\n",
    "\n",
    "            for j in range(self.num_ve_iterations):\n",
    "                dist_square = torch.tensor(0.)\n",
    "                for i in range(self.tasks_per_meta_batch):\n",
    "                    task_weights, _ = self.inner_loop(tasks_list[i], task_weights_list[i])\n",
    "                    task_weights_list[i] = task_weights\n",
    "                    dist_square += sum(list(map(lambda p: torch.sum(torch.square(p[1] - p[0])), zip(task_weights, self.weights))))\n",
    "                \n",
    "                d = torch.sqrt(dist_square)\n",
    "                r = self.radius\n",
    "                #print(\"Before Projection\", j, dist_square, torch.sqrt(dist_square))\n",
    "                \n",
    "                if d > r:\n",
    "                    for i in range(self.tasks_per_meta_batch):\n",
    "                        task_weights_list[i] = list(map(lambda p: (r*p[0] + (d-r)*p[1])/d, zip(task_weights_list[i], self.weights)))\n",
    "                \n",
    "            for i in range(self.tasks_per_meta_batch):\n",
    "                _, loss = self.inner_loop(tasks_list[i], task_weights_list[i], True)\n",
    "                meta_loss += loss\n",
    "            \n",
    "            # compute meta gradient of loss with respect to maml weights\n",
    "            meta_grads = torch.autograd.grad(meta_loss, self.weights)\n",
    "            \n",
    "            # assign meta gradient to weights and take optimisation step\n",
    "            for w, g in zip(self.weights, meta_grads):\n",
    "                w.grad = g\n",
    "            self.meta_optimiser.step()\n",
    "            \n",
    "            # log metrics\n",
    "            epoch_loss += meta_loss.item() / self.tasks_per_meta_batch\n",
    "            \n",
    "            if iteration % self.print_every == 0:\n",
    "                print(\"{}/{}. loss: {}\".format(iteration, num_iterations, epoch_loss / self.plot_every))\n",
    "            \n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(epoch_loss / self.plot_every)\n",
    "                epoch_loss = 0\n",
    "   \n",
    "            if iteration % self.save_every == 0:\n",
    "                torch.save(maml.model.state_dict(), './vmaml.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e2155456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = Sine_Task_Distribution(0.1, 5, 0, np.pi, -5, 5)\n",
    "maml = VMAML(MAMLModel(), tasks, inner_lr=0.01, meta_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a76d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10000. loss: 0.414044808959961\n",
      "20/10000. loss: 0.38304346923828125\n",
      "30/10000. loss: 0.36935836486816404\n",
      "40/10000. loss: 0.37011560668945315\n",
      "50/10000. loss: 0.3614697509765624\n",
      "60/10000. loss: 0.3535485443115235\n",
      "70/10000. loss: 0.34998448791503906\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(222)\n",
    "torch.cuda.manual_seed_all(222)\n",
    "np.random.seed(222)\n",
    "maml.main_loop(num_iterations=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
